{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS345 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset, we will be using the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). Due to the size of this dataset, it could not be uploaded to github. Please download the dataset yourself, extract it, and move it to the \"data\" directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'pos' reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pos files: 100%|██████████| 12500/12500 [00:02<00:00, 4183.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'neg' reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading neg files: 100%|██████████| 12500/12500 [00:02<00:00, 4675.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 reviews from 'data/aclImdb/train'\n",
      "Processing 'pos' reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pos files: 100%|██████████| 12500/12500 [00:02<00:00, 4676.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'neg' reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading neg files: 100%|██████████| 12500/12500 [00:02<00:00, 4456.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 reviews from 'data/aclImdb/test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir):\n",
    "    data = []\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        sentiment_dir = os.path.join(data_dir, sentiment)\n",
    "        print(f\"Processing '{sentiment}' reviews...\")\n",
    "        file_list = os.listdir(sentiment_dir)\n",
    "        # Use tqdm to create a progress bar as loading can take a while, we want to make sure it isn't hanging\n",
    "        for filename in tqdm(file_list, desc=f\"Loading {sentiment} files\"):\n",
    "            if filename.endswith('.txt'):\n",
    "                filepath = os.path.join(sentiment_dir, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    review = f.read()\n",
    "                data.append({\n",
    "                    'review': review,\n",
    "                    'sentiment': 1 if sentiment == 'pos' else 0,\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} reviews from '{data_dir}'\")\n",
    "    return df\n",
    "\n",
    "train_data = load_data('data/aclImdb/train')\n",
    "test_data = load_data('data/aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...          1\n",
      "1  Homelessness (or Houselessness as George Carli...          1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...          1\n",
      "3  This is easily the most underrated film inn th...          1\n",
      "4  This is not the typical Mel Brooks film. It wa...          1\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jaxfu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jaxfu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Preprocessing text: 100%|██████████| 25000/25000 [00:19<00:00, 1263.22it/s]\n",
      "Preprocessing text: 100%|██████████| 25000/25000 [00:18<00:00, 1358.68it/s]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text, stop_words, lemmatizer):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_with_nltk(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tqdm.pandas(desc=\"Preprocessing text\")\n",
    "    data['review'] = data['review'].progress_apply(lambda x: preprocess_text(x, stop_words, lemmatizer))\n",
    "    return data\n",
    "\n",
    "train_data = preprocess_with_nltk(train_data)\n",
    "test_data = preprocess_with_nltk(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the data to what we had before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  bromwell high cartoon comedy ran time program ...          1\n",
      "1  homelessness houselessness george carlin state...          1\n",
      "2  brilliant overacting lesley ann warren best dr...          1\n",
      "3  easily underrated film inn brook cannon sure f...          1\n",
      "4  typical mel brook film much less slapstick mov...          1\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
